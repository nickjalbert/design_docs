(env) nickj@roflwin:~/agentos/example_agent$ cat main.py
# This file was auto-generated by `agentos init` on Feb 08, 2021 13:18:38.
import agentos
import random
import gym


# policy?
# policy? trainer?



# * Making it easy to compose non-core agentos environments and policies and
# trainers and etc
#
#   * env - 2048
#
#   * env - mountain car (from gym)
#
#   * TODO - refactor example agents into components and move them out?  maybe
#   with a "recipe" system to quickly reconstitute intersting agents


# Agent
#  - environment (acs)
#  - policy (acs): policy is stateful
#  - trainer (acs)
#  - glue code (agent)


my_agent/
  - main.py
  - trainer.py
  - environment.py
  - policy.py
  - policy/
    - serialized_nn.out
  - agent.ini



      [Agent]
      class = main.BatchAgent

      [Policy] # self.policy
      class = policy.DeepQNetwork
      architecture = [10,100,100]
      storage = ./policy/

      [Environment] # self.environment
      class = environment.Cartpole
      max_steps = 200

      [Trainer] # self.trainer
      class = trainer.ReplayBufferSGD
      buffer_size = 10000
      batch_size = 100






# agentos train /path/to/agent.ini  1000 # agent.ini - points to an env class, a stateful instantiation of a policy class, trainer , agent
# agentos run /path/to/agent.ini   # max_iters, spits out current performance
# agentos train /path/to/agent.ini  1000  -- storing memory
# agentos run /path/to/agent.ini   # max_iters, spits out current performance
# agentos reset /path/to/agent.ini  # resets all stateful memory

# user story for training and using agents
#  - two different worlds: online and batch, probably different APIs


class BatchAgent(agentos.Agent):
    def train():
        self.policy = self.trainer.train(self.policy, self.environment)

    def advance():
        next_action = self.policy.decide(obs)
        obs = self.environment.step(next_action)


class DeepQNetwork(agentos.Policy):
    def decide(self, obs):
        action_probabilites = self.nn(obs)
        return random.weighted_choose(action_probabilities)

class ReplayBufferSGD(agentos.Trainer):
    def train(policy, environment_class):
        rollouts = agentos.do_rollouts(policy, environment_class)
        advantaged_rollouts = calculate_advantage(rollouts)

        # This is a little weird (reaching into policy)
        policy.nn.train(advantaged_rollouts)



# TODO - a little klugy (see BatchAgent as well).
class OnlineAgent(agentos.Agent):
    def train():
        pass

    def advance():
        next_action = self.policy.decide(self.old_obs)
        obs, done, reward, _ = self.environment.step(next_action)
        self.policy = self.trainer.train(policy, copy.deepcopy(self.environment))


class Trainer:
    def train(self, policy, environment):
        # rollouts in the env (complicated in the online agent)



# A minimal example agent class.
class MyAgent(agentos.Agent):
    # train(policy, *args) -> policy
    def train(self):
        #if self.should_train:
        #    self.trainer.train()
        self.trainer


    def advance(self):
        next_action = self.policy.decide(obs)
        obs = self.environment.step(next_action)




if __name__ == "__main__":
    # agentos.run_agent(MyAgent) # See the defaults defined by agent.ini

    # agentos run  # use defaults from agent.ini
    # agentos run -a ../../agent.ini
    # agentos run -e myenv.Env -p mypolicy.Policy


    agentos.run_agent_file('path/to/file/agent.ini') # how does it know how to find the agent.ini?

    agentos.run_agent(agent=MyAgent, environment=MyEnv, policy=MyPolicy, trainer=MyTrainer) # additional_args: max_iters, hz


    #agentos.run_agent(MyAgent, acs.environment, max_iters=5)
